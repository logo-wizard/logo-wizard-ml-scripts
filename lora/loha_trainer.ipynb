{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import toml\n",
    "from google.colab import drive\n",
    "from accelerate.utils import write_basic_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "project_name = \"loha_v2\"\n",
    "model_url = \"https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-nonema-pruned.safetensors\"\n",
    "is_based_on_sd2 = True\n",
    "\n",
    "# Processing\n",
    "resolution = 768\n",
    "flip_aug = True\n",
    "\n",
    "# Steps\n",
    "num_repeats = 5\n",
    "max_train_steps = 5000\n",
    "save_every_n_epochs = 5\n",
    "keep_only_last_n_epochs = 10\n",
    "train_batch_size = 2\n",
    "unet_lr = 2e-4\n",
    "text_encoder_lr = 1e-6\n",
    "lr_scheduler = \"cosine_with_restarts\"\n",
    "lr_scheduler_num_cycles = 3\n",
    "min_snr_gamma = True\n",
    "min_snr_gamma_value = 5.0 if min_snr_gamma else None\n",
    "\n",
    "# Structure\n",
    "lora_type = \"LoHa Lycoris\"\n",
    "network_dim = 8\n",
    "network_alpha = 4\n",
    "conv_dim = 8\n",
    "conv_alpha = 1\n",
    "conv_compression = False\n",
    "network_module = \"lycoris.kohya\" if \"Lycoris\" in lora_type else \"networks.lora\"\n",
    "network_args = [\n",
    "    f\"conv_dim={conv_dim}\",\n",
    "    f\"conv_alpha={conv_alpha}\",\n",
    "    f\"algo={'loha' if 'LoHa' in lora_type else 'lora'}\",\n",
    "    f\"disable_conv_cp={str(not conv_compression)}\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции для установки зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/content\"\n",
    "deps_dir = os.path.join(root_dir, \"deps\")\n",
    "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
    "\n",
    "main_dir = os.path.join(root_dir, \"drive/MyDrive/Loras\")\n",
    "log_folder = os.path.join(main_dir, \"logs\")\n",
    "config_folder = os.path.join(main_dir, project_name)\n",
    "images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
    "output_folder = os.path.join(main_dir, project_name, \"output\")\n",
    "\n",
    "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
    "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
    "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
    "\n",
    "\n",
    "def clone_repo():\n",
    "    os.chdir(root_dir)\n",
    "    os.system(f\"git clone https://github.com/kohya-ss/sd-scripts {repo_dir}\")\n",
    "    os.chdir(repo_dir)\n",
    "    commit = \"5050971ac687dca70ba0486a583d283e8ae324e2\"\n",
    "    os.system(f\"git reset --hard {commit}\")\n",
    "    os.system(\n",
    "        \"wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/requirements.txt -q -O requirements.txt\"\n",
    "    )\n",
    "\n",
    "\n",
    "def install_dependencies():\n",
    "    clone_repo()\n",
    "    os.system(\"apt -y update -qq\")\n",
    "    os.system(\"apt -y install aria2\")\n",
    "    os.system(\"pip -q install --upgrade -r requirements.txt\")\n",
    "\n",
    "    os.system('sed -i \"s@cpu@cuda@\" library/model_util.py')\n",
    "\n",
    "    os.system(\n",
    "        \"sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py\"\n",
    "    )\n",
    "\n",
    "    os.system(\"sed -i 's/{:06d}/{:02d}/g' library/train_util.py\")\n",
    "    os.system(\n",
    "        'sed -i \\'s/model_name + \".\"/model_name + \"-{:02d}.\".format(num_train_epochs)/g\\' train_network.py'\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(accelerate_config_file):\n",
    "        write_basic_config(save_location=accelerate_config_file)\n",
    "\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "    os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для создания файлов конфигурации для датасета и модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(model_file, dataset_config_file, config_file):\n",
    "    config_dict = {\n",
    "        \"additional_network_arguments\": {\n",
    "            \"unet_lr\": unet_lr,\n",
    "            \"text_encoder_lr\": text_encoder_lr,\n",
    "            \"network_dim\": network_dim,\n",
    "            \"network_alpha\": network_alpha,\n",
    "            \"network_module\": network_module,\n",
    "            \"network_args\": network_args,\n",
    "            \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
    "            \"network_weights\": None,\n",
    "        },\n",
    "        \"optimizer_arguments\": {\n",
    "            \"learning_rate\": unet_lr,\n",
    "            \"lr_scheduler\": lr_scheduler,\n",
    "            \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles,\n",
    "            \"lr_scheduler_power\": None,\n",
    "            \"lr_warmup_steps\": None,\n",
    "            \"optimizer_type\": \"AdamW8bit\",\n",
    "            \"optimizer_args\": None,\n",
    "        },\n",
    "        \"training_arguments\": {\n",
    "            \"max_train_steps\": max_train_steps,\n",
    "            \"max_train_epochs\": None,\n",
    "            \"save_every_n_epochs\": save_every_n_epochs,\n",
    "            \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
    "            \"train_batch_size\": train_batch_size,\n",
    "            \"noise_offset\": None,\n",
    "            \"clip_skip\": 2,\n",
    "            \"min_snr_gamma\": min_snr_gamma_value,\n",
    "            \"weighted_captions\": False,\n",
    "            \"seed\": 42,\n",
    "            \"max_token_length\": 225,\n",
    "            \"xformers\": True,\n",
    "            \"lowram\": True,\n",
    "            \"max_data_loader_n_workers\": 8,\n",
    "            \"persistent_data_loader_workers\": True,\n",
    "            \"save_precision\": \"fp16\",\n",
    "            \"mixed_precision\": \"fp16\",\n",
    "            \"output_dir\": output_folder,\n",
    "            \"logging_dir\": log_folder,\n",
    "            \"output_name\": project_name,\n",
    "            \"log_prefix\": project_name,\n",
    "            \"save_state\": False,\n",
    "            \"save_last_n_epochs_state\": None,\n",
    "            \"resume\": None,\n",
    "        },\n",
    "        \"model_arguments\": {\n",
    "            \"pretrained_model_name_or_path\": model_file,\n",
    "            \"v2\": is_based_on_sd2,\n",
    "            \"v_parameterization\": True if is_based_on_sd2 else None,\n",
    "        },\n",
    "        \"saving_arguments\": {\n",
    "            \"save_model_as\": \"safetensors\",\n",
    "        },\n",
    "        \"dreambooth_arguments\": {\n",
    "            \"prior_loss_weight\": 1.0,\n",
    "        },\n",
    "        \"dataset_arguments\": {\n",
    "            \"cache_latents\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for key in config_dict:\n",
    "        if isinstance(config_dict[key], dict):\n",
    "            config_dict[key] = {\n",
    "                k: v for k, v in config_dict[key].items() if v is not None\n",
    "            }\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "        f.write(toml.dumps(config_dict))\n",
    "    print(f\"\\nConfig saved to {config_file}\")\n",
    "\n",
    "    dataset_config_dict = {\n",
    "        \"general\": {\n",
    "            \"resolution\": resolution,\n",
    "            \"shuffle_caption\": False,\n",
    "            \"keep_tokens\": 0,\n",
    "            \"flip_aug\": flip_aug,\n",
    "            \"caption_extension\": \".txt\",\n",
    "            \"enable_bucket\": True,\n",
    "            \"bucket_reso_steps\": 64,\n",
    "            \"bucket_no_upscale\": False,\n",
    "            \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
    "            \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
    "        },\n",
    "        \"datasets\": [\n",
    "            {\n",
    "                \"subsets\": [\n",
    "                    {\n",
    "                        \"num_repeats\": num_repeats,\n",
    "                        \"image_dir\": images_folder,\n",
    "                        \"class_tokens\": None,\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for key in dataset_config_dict:\n",
    "        if isinstance(dataset_config_dict[key], dict):\n",
    "            dataset_config_dict[key] = {\n",
    "                k: v for k, v in dataset_config_dict[key].items() if v is not None\n",
    "            }\n",
    "\n",
    "    with open(dataset_config_file, \"w\") as f:\n",
    "        f.write(toml.dumps(dataset_config_dict))\n",
    "    print(f\"Dataset config saved to {dataset_config_file}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для скачивания базовой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(model_url):\n",
    "    model_url = model_url.strip()\n",
    "    model_file = f\"/content{model_url[model_url.rfind('/'):]}\"\n",
    "\n",
    "    os.system(\n",
    "        f'aria2c \"{model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"'\n",
    "    )\n",
    "\n",
    "    return model_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основная функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if not os.path.exists(\"/content/drive\"):\n",
    "        print(\"Connecting to Google Drive...\")\n",
    "        drive.mount(\"/content/drive\")\n",
    "\n",
    "    for dir in (main_dir, deps_dir, repo_dir, log_folder, output_folder, config_folder):\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\nInstalling dependencies...\\n\")\n",
    "    install_dependencies()\n",
    "    print(f\"\\nInstallation finished.\")\n",
    "\n",
    "    print(\"\\nDownloading model...\")\n",
    "    model_file = download_model(model_url=model_url)\n",
    "    print(\"\\nDownloading finished.\")\n",
    "\n",
    "    create_config(\"\\nCreating config files...\\n\")\n",
    "    create_config(model_file=model_file, dataset_config_file=dataset_config_file, config_file=config_file)\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    os.chdir(repo_dir)\n",
    "\n",
    "    os.system(\n",
    "        f\"accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network.py --dataset_config={dataset_config_file} --config_file={config_file}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
